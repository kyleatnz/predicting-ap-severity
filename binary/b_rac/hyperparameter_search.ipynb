{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e983d-e93e-4914-9f36-379aa98a5ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48cbcbb-fb1d-4006-97a2-9dc94003ac1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../psap_utils\")\n",
    "from psap_utils import find_best_params_average\n",
    "from psap_utils import print_results_in_separate_lines\n",
    "from metrics import weighted_balanced_accuracy\n",
    "# 机器学习模型库\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 其他工具库\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 评估和模型选择库\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, roc_auc_score, make_scorer, accuracy_score, f1_score, fbeta_score, matthews_corrcoef, balanced_accuracy_score, brier_score_loss, class_likelihood_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6601a80c-f825-4ec7-8f6a-0e432aa8e2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('b_rac_splits.pkl', 'rb') as f:\n",
    "    datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0a7e852-a657-4c20-84f3-725fa1932d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring= {\n",
    "    'acc': make_scorer(accuracy_score),\n",
    "    #'auc' make_scorer(roc_auc_score, needs_proba=True, average='macro'),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    #'wba': make_scorer(weighted_balanced_accuracy),\n",
    "    'ba': make_scorer(balanced_accuracy_score),\n",
    "    #'f1': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b101fdab-55fb-4669-8a90-bf40c9c7f33b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"LR\", LogisticRegression(max_iter=400), {\n",
    "        'solver': ['newton-cg', 'lbfgs', 'sag'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'penalty': ['l2', 'none']\n",
    "    }),\n",
    "    (\"SVM\", SVC(probability=True), {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "        'kernel': ['linear', 'rbf', 'poly']\n",
    "    }),\n",
    "    (\"RF\", RandomForestClassifier(criterion='gini', min_samples_split=2, bootstrap=True), {\n",
    "        'n_estimators': [50, 100, 150, 200], \n",
    "        'max_depth': [3, 4, 5]\n",
    "    }),\n",
    "    (\"XGB\", XGBClassifier(booster='gbtree'), {\n",
    "        'n_estimators': [50, 100, 150, 200], \n",
    "        'max_depth': [3, 4, 5], \n",
    "        'learning_rate': [0.001, 0.01, 0.1]\n",
    "    }),\n",
    "    (\"ANN\", MLPClassifier(hidden_layer_sizes=(42, 21,), early_stopping=False), {\n",
    "        'hidden_layer_sizes': [(42, 21,), (21, 14,), (42, 21, 14,)],\n",
    "        'activation': ['tanh'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.001, 0.01, 0.1],\n",
    "        'learning_rate': ['adaptive'],\n",
    "        'max_iter': [200, 300, 400]\n",
    "    })]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7a4d23-34fe-4385-ba53-dfde9e239403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.09s/dataset]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LR:\n",
      "  acc:\n",
      "    best_params: {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "    best_score: 0.81984496124031\n",
      "\n",
      "  auc:\n",
      "    best_params: {'C': 0.001, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "    best_score: 0.9000560712813839\n",
      "\n",
      "  mcc:\n",
      "    best_params: {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "    best_score: 0.6543402764980939\n",
      "\n",
      "  wba:\n",
      "    best_params: {'C': 0.001, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "    best_score: 0.8070209619270081\n",
      "\n",
      "  ba:\n",
      "    best_params: {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "    best_score: 0.8297667309837214\n",
      "\n",
      "  f1:\n",
      "    best_params: {'C': 0.001, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "    best_score: 0.8282139692749058\n",
      "\n",
      "Processing SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|████████████████████████████████████████████████████████| 10/10 [04:25<00:00, 26.55s/dataset]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM:\n",
      "  acc:\n",
      "    best_params: {'C': 1000, 'kernel': 'linear'}\n",
      "    best_score: 0.8158139534883722\n",
      "\n",
      "  auc:\n",
      "    best_params: {'C': 1000, 'kernel': 'linear'}\n",
      "    best_score: 0.8969097644590288\n",
      "\n",
      "  mcc:\n",
      "    best_params: {'C': 10, 'kernel': 'linear'}\n",
      "    best_score: 0.6748275694485353\n",
      "\n",
      "  wba:\n",
      "    best_params: {'C': 1000, 'kernel': 'linear'}\n",
      "    best_score: 0.7927965425425756\n",
      "\n",
      "  ba:\n",
      "    best_params: {'C': 100, 'kernel': 'linear'}\n",
      "    best_score: 0.8335938950604092\n",
      "\n",
      "  f1:\n",
      "    best_params: {'C': 1000, 'kernel': 'linear'}\n",
      "    best_score: 0.8151472253899918\n",
      "\n",
      "Processing RF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.18s/dataset]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RF:\n",
      "  acc:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.8338501291989664\n",
      "\n",
      "  auc:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.9243094284437918\n",
      "\n",
      "  mcc:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.6944740367426914\n",
      "\n",
      "  wba:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.8139472564933422\n",
      "\n",
      "  ba:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.8484577008650648\n",
      "\n",
      "  f1:\n",
      "    best_params: {'max_depth': 5, 'n_estimators': 200}\n",
      "    best_score: 0.836255262517865\n",
      "\n",
      "Processing XGB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|████████████████████████████████████████████████████████| 10/10 [00:48<00:00,  4.87s/dataset]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGB:\n",
      "  acc:\n",
      "    best_params: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}\n",
      "    best_score: 0.8499224806201552\n",
      "\n",
      "  auc:\n",
      "    best_params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "    best_score: 0.9264807373798936\n",
      "\n",
      "  mcc:\n",
      "    best_params: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 150}\n",
      "    best_score: 0.7010341704114292\n",
      "\n",
      "  wba:\n",
      "    best_params: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50}\n",
      "    best_score: 0.8476624381544238\n",
      "\n",
      "  ba:\n",
      "    best_params: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 200}\n",
      "    best_score: 0.8536397856208306\n",
      "\n",
      "  f1:\n",
      "    best_params: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}\n",
      "    best_score: 0.8641692701976785\n",
      "\n",
      "Processing ANN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|████████████████████████████████████████████████████████| 10/10 [04:08<00:00, 24.87s/dataset]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for ANN:\n",
      "  acc:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (42, 21), 'learning_rate': 'adaptive', 'max_iter': 300, 'solver': 'sgd'}\n",
      "    best_score: 0.8073385012919896\n",
      "\n",
      "  auc:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (21, 14), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "    best_score: 0.8785122621790166\n",
      "\n",
      "  mcc:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (42, 21), 'learning_rate': 'adaptive', 'max_iter': 300, 'solver': 'sgd'}\n",
      "    best_score: 0.6433043899179061\n",
      "\n",
      "  wba:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (42, 21), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "    best_score: 0.7916115331993059\n",
      "\n",
      "  ba:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (42, 21), 'learning_rate': 'adaptive', 'max_iter': 300, 'solver': 'sgd'}\n",
      "    best_score: 0.8221997532443748\n",
      "\n",
      "  f1:\n",
      "    best_params: {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (42, 21), 'learning_rate': 'adaptive', 'max_iter': 200, 'solver': 'adam'}\n",
      "    best_score: 0.8128708154981801\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_best_params = {}\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model, params in models:\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    \n",
    "    # Call the function for finding the best parameters on average\n",
    "    best_params = find_best_params_average(datasets=datasets, \n",
    "                                           model=model, \n",
    "                                           param_grid=params, \n",
    "                                           cv_folds=5,  # or another number if you prefer\n",
    "                                           scoring=scoring)\n",
    "    \n",
    "    # Save the results associated with the model\n",
    "    all_best_params[model_name] = best_params\n",
    "\n",
    "    print_results_in_separate_lines(model_name,best_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b1635c-8afa-40e2-a16a-c8256ed62319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('b_rac_all_best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(all_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff8aaa-f9ae-46d1-938b-503327442145",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('b_rac_all_best_params.pkl', 'rb') as f:\n",
    "    all_best_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e583121d-73cc-4970-ac3b-859e44e56c6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': {'acc': {'best_params': {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "   'best_score': 0.81984496124031},\n",
       "  'auc': {'best_params': {'C': 0.001,\n",
       "    'penalty': 'none',\n",
       "    'solver': 'newton-cg'},\n",
       "   'best_score': 0.9000560712813839},\n",
       "  'mcc': {'best_params': {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "   'best_score': 0.6543402764980939},\n",
       "  'wba': {'best_params': {'C': 0.001,\n",
       "    'penalty': 'none',\n",
       "    'solver': 'newton-cg'},\n",
       "   'best_score': 0.8070209619270081},\n",
       "  'ba': {'best_params': {'C': 1000, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "   'best_score': 0.8297667309837214},\n",
       "  'f1': {'best_params': {'C': 0.001, 'penalty': 'none', 'solver': 'newton-cg'},\n",
       "   'best_score': 0.8282139692749058}},\n",
       " 'SVM': {'acc': {'best_params': {'C': 1000, 'kernel': 'linear'},\n",
       "   'best_score': 0.8158139534883722},\n",
       "  'auc': {'best_params': {'C': 1000, 'kernel': 'linear'},\n",
       "   'best_score': 0.8969097644590288},\n",
       "  'mcc': {'best_params': {'C': 10, 'kernel': 'linear'},\n",
       "   'best_score': 0.6748275694485353},\n",
       "  'wba': {'best_params': {'C': 1000, 'kernel': 'linear'},\n",
       "   'best_score': 0.7927965425425756},\n",
       "  'ba': {'best_params': {'C': 100, 'kernel': 'linear'},\n",
       "   'best_score': 0.8335938950604092},\n",
       "  'f1': {'best_params': {'C': 1000, 'kernel': 'linear'},\n",
       "   'best_score': 0.8151472253899918}},\n",
       " 'RF': {'acc': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.8338501291989664},\n",
       "  'auc': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.9243094284437918},\n",
       "  'mcc': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.6944740367426914},\n",
       "  'wba': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.8139472564933422},\n",
       "  'ba': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.8484577008650648},\n",
       "  'f1': {'best_params': {'max_depth': 5, 'n_estimators': 200},\n",
       "   'best_score': 0.836255262517865}},\n",
       " 'XGB': {'acc': {'best_params': {'learning_rate': 0.01,\n",
       "    'max_depth': 4,\n",
       "    'n_estimators': 100},\n",
       "   'best_score': 0.8499224806201552},\n",
       "  'auc': {'best_params': {'learning_rate': 0.1,\n",
       "    'max_depth': 3,\n",
       "    'n_estimators': 50},\n",
       "   'best_score': 0.9264807373798936},\n",
       "  'mcc': {'best_params': {'learning_rate': 0.01,\n",
       "    'max_depth': 4,\n",
       "    'n_estimators': 150},\n",
       "   'best_score': 0.7010341704114292},\n",
       "  'wba': {'best_params': {'learning_rate': 0.01,\n",
       "    'max_depth': 4,\n",
       "    'n_estimators': 50},\n",
       "   'best_score': 0.8476624381544238},\n",
       "  'ba': {'best_params': {'learning_rate': 0.01,\n",
       "    'max_depth': 4,\n",
       "    'n_estimators': 200},\n",
       "   'best_score': 0.8536397856208306},\n",
       "  'f1': {'best_params': {'learning_rate': 0.01,\n",
       "    'max_depth': 4,\n",
       "    'n_estimators': 100},\n",
       "   'best_score': 0.8641692701976785}},\n",
       " 'ANN': {'acc': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.01,\n",
       "    'hidden_layer_sizes': (42, 21),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 300,\n",
       "    'solver': 'sgd'},\n",
       "   'best_score': 0.8073385012919896},\n",
       "  'auc': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.001,\n",
       "    'hidden_layer_sizes': (21, 14),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 200,\n",
       "    'solver': 'adam'},\n",
       "   'best_score': 0.8785122621790166},\n",
       "  'mcc': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.01,\n",
       "    'hidden_layer_sizes': (42, 21),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 300,\n",
       "    'solver': 'sgd'},\n",
       "   'best_score': 0.6433043899179061},\n",
       "  'wba': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.1,\n",
       "    'hidden_layer_sizes': (42, 21),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 200,\n",
       "    'solver': 'adam'},\n",
       "   'best_score': 0.7916115331993059},\n",
       "  'ba': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.01,\n",
       "    'hidden_layer_sizes': (42, 21),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 300,\n",
       "    'solver': 'sgd'},\n",
       "   'best_score': 0.8221997532443748},\n",
       "  'f1': {'best_params': {'activation': 'tanh',\n",
       "    'alpha': 0.1,\n",
       "    'hidden_layer_sizes': (42, 21),\n",
       "    'learning_rate': 'adaptive',\n",
       "    'max_iter': 200,\n",
       "    'solver': 'adam'},\n",
       "   'best_score': 0.8128708154981801}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d0e5e2d-7281-4abe-aeec-b3dfc37f579d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_parameters = {}\n",
    "\n",
    "# Extracting and deduplicating parameters\n",
    "for model_name, metrics in all_best_params.items():\n",
    "    for metric, content in metrics.items():\n",
    "        params = content['best_params']\n",
    "        if model_name not in unique_parameters:\n",
    "            unique_parameters[model_name] = set()  # Initialize as a new set\n",
    "        unique_parameters[model_name].add(frozenset(params.items()))  # Now you can safely add the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd5b336b-9d8b-4ba5-b2f7-dd23110df540",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': {frozenset({('C', 1000), ('penalty', 'l2'), ('solver', 'lbfgs')}),\n",
       "  frozenset({('C', 0.001), ('penalty', 'none'), ('solver', 'newton-cg')})},\n",
       " 'SVM': {frozenset({('C', 10), ('kernel', 'linear')}),\n",
       "  frozenset({('C', 100), ('kernel', 'linear')}),\n",
       "  frozenset({('C', 1000), ('kernel', 'linear')})},\n",
       " 'RF': {frozenset({('max_depth', 5), ('n_estimators', 200)})},\n",
       " 'XGB': {frozenset({('learning_rate', 0.01),\n",
       "             ('max_depth', 4),\n",
       "             ('n_estimators', 150)}),\n",
       "  frozenset({('learning_rate', 0.01),\n",
       "             ('max_depth', 4),\n",
       "             ('n_estimators', 100)}),\n",
       "  frozenset({('learning_rate', 0.1), ('max_depth', 3), ('n_estimators', 50)}),\n",
       "  frozenset({('learning_rate', 0.01),\n",
       "             ('max_depth', 4),\n",
       "             ('n_estimators', 200)}),\n",
       "  frozenset({('learning_rate', 0.01),\n",
       "             ('max_depth', 4),\n",
       "             ('n_estimators', 50)})},\n",
       " 'ANN': {frozenset({('activation', 'tanh'),\n",
       "             ('alpha', 0.1),\n",
       "             ('hidden_layer_sizes', (42, 21)),\n",
       "             ('learning_rate', 'adaptive'),\n",
       "             ('max_iter', 200),\n",
       "             ('solver', 'adam')}),\n",
       "  frozenset({('activation', 'tanh'),\n",
       "             ('alpha', 0.01),\n",
       "             ('hidden_layer_sizes', (42, 21)),\n",
       "             ('learning_rate', 'adaptive'),\n",
       "             ('max_iter', 300),\n",
       "             ('solver', 'sgd')}),\n",
       "  frozenset({('activation', 'tanh'),\n",
       "             ('alpha', 0.001),\n",
       "             ('hidden_layer_sizes', (21, 14)),\n",
       "             ('learning_rate', 'adaptive'),\n",
       "             ('max_iter', 200),\n",
       "             ('solver', 'adam')})}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e78393a9-75dc-41ae-8cad-955632003a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone  # 用于复制模型实例\n",
    "\n",
    "# 初始化字典，每个模型名对应一个空列表\n",
    "configured_models = {model_name: [] for model_name, _, _ in models}\n",
    "\n",
    "for model_name, model_instance, _ in models:\n",
    "    # 检查确保有为该模型定义的唯一参数\n",
    "    if model_name in unique_parameters:\n",
    "        for param in unique_parameters[model_name]:\n",
    "            # 复制原始模型实例以避免修改它\n",
    "            model_copy = clone(model_instance)\n",
    "            \n",
    "            # 更新模型参数\n",
    "            model_copy.set_params(**dict(param))  # 将 frozenset 转换为字典\n",
    "            \n",
    "            # 将配置过的模型实例添加到列表中\n",
    "            configured_models[model_name].append(model_copy)\n",
    "\n",
    "# 现在，configured_models 字典中的每个条目都是对应模型的配置实例列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7123ff34-66e5-4f95-a30a-2179fd9631b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': [LogisticRegression(C=1000, max_iter=400),\n",
       "  LogisticRegression(C=0.001, max_iter=400, penalty='none', solver='newton-cg')],\n",
       " 'SVM': [SVC(C=10, kernel='linear', probability=True),\n",
       "  SVC(C=100, kernel='linear', probability=True),\n",
       "  SVC(C=1000, kernel='linear', probability=True)],\n",
       " 'RF': [RandomForestClassifier(max_depth=5, n_estimators=200)],\n",
       " 'XGB': [XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "  XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "  XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=50, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "  XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "  XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=50, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...)],\n",
       " 'ANN': [MLPClassifier(activation='tanh', alpha=0.1, hidden_layer_sizes=(42, 21),\n",
       "                learning_rate='adaptive'),\n",
       "  MLPClassifier(activation='tanh', alpha=0.01, hidden_layer_sizes=(42, 21),\n",
       "                learning_rate='adaptive', max_iter=300, solver='sgd'),\n",
       "  MLPClassifier(activation='tanh', alpha=0.001, hidden_layer_sizes=(21, 14),\n",
       "                learning_rate='adaptive')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configured_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bf0cd-a1ce-4131-b003-7bc449182035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/data_rac.csv')\n",
    "y = data['RAC']\n",
    "prevalence = y.sum()/y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98f7b5e4-04ee-477f-bd47-8578f023ddea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 假设 datasets 是一个包含所有数据集的列表，每个数据集都是一个字典，包含 'X_train', 'y_train', 'X_test', 'y_test'\n",
    "for model_name, model_list in configured_models.items():\n",
    "    for index, model in enumerate(model_list):  # 添加索引以区分不同的模型实例\n",
    "        unique_model_name = f\"{model_name}: model{index + 1}\"  # 为每个模型创建一个唯一的名称，如 \"LR: 模型1\"\n",
    "        scores = {key: [] for key in scoring}\n",
    "\n",
    "        for dataset in datasets:\n",
    "            X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n",
    "\n",
    "            # 训练模型并做出预测\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]  # 获取属于正类的概率\n",
    "            else:\n",
    "                y_proba = None\n",
    "\n",
    "            for name, scorer in scoring.items():\n",
    "                if 'auc' in name and y_proba is not None:\n",
    "                    score_value = scorer._score_func(y_test, y_proba, **scorer._kwargs)\n",
    "                elif \n",
    "                else:\n",
    "                    score_value = scorer._score_func(y_test, y_pred, **scorer._kwargs)\n",
    "                scores[name].append(score_value)\n",
    "\n",
    "        # 计算每个评分指标的平均值\n",
    "        average_scores = {key: np.mean(value) for key, value in scores.items()}\n",
    "\n",
    "        # 将结果保存在 'results' 字典中，使用模型的唯一名称作为键\n",
    "        results[unique_model_name] = average_scores\n",
    "\n",
    "# 'results' 字典现在包含每个模型实例的评分，每个实例都有一个唯一的名称\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5548a812-a91f-4d61-bf83-bc8b5f64df3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR: 模型1': {'acc': 0.8212074303405572,\n",
       "  'auc': 0.9026479903034096,\n",
       "  'mcc': 0.6565601267770683,\n",
       "  'wba': 0.8076673443853613,\n",
       "  'ba': 0.8309303644041289,\n",
       "  'f1': 0.828930406139046},\n",
       " 'LR: 模型2': {'acc': 0.8199690402476779,\n",
       "  'auc': 0.9033420003127933,\n",
       "  'mcc': 0.6511752099851924,\n",
       "  'wba': 0.8081782400166823,\n",
       "  'ba': 0.828435838285893,\n",
       "  'f1': 0.8292465717864139},\n",
       " 'SVM: 模型1': {'acc': 0.8071207430340557,\n",
       "  'auc': 0.8992160619330625,\n",
       "  'mcc': 0.6726009713998958,\n",
       "  'wba': 0.7753167031592118,\n",
       "  'ba': 0.8299587503909915,\n",
       "  'f1': 0.7972027200261401},\n",
       " 'SVM: 模型2': {'acc': 0.8139318885448917,\n",
       "  'auc': 0.9005605841413825,\n",
       "  'mcc': 0.6716250468421313,\n",
       "  'wba': 0.7870855489521427,\n",
       "  'ba': 0.8332098451673444,\n",
       "  'f1': 0.8095462817698154},\n",
       " 'SVM: 模型3': {'acc': 0.8181114551083593,\n",
       "  'auc': 0.9010175555208008,\n",
       "  'mcc': 0.6730177795180806,\n",
       "  'wba': 0.7939370242936085,\n",
       "  'ba': 0.8354707538317172,\n",
       "  'f1': 0.8163902527508322},\n",
       " 'RF: 模型1': {'acc': 0.8301857585139321,\n",
       "  'auc': 0.9295169299343135,\n",
       "  'mcc': 0.6872265103729343,\n",
       "  'wba': 0.8100276300698571,\n",
       "  'ba': 0.844661010322177,\n",
       "  'f1': 0.8324097986791278},\n",
       " 'XGB: 模型1': {'acc': 0.8529411764705882,\n",
       "  'auc': 0.9316742258367219,\n",
       "  'mcc': 0.706312417539059,\n",
       "  'wba': 0.8490316442498175,\n",
       "  'ba': 0.855748553331248,\n",
       "  'f1': 0.8661248274254083},\n",
       " 'XGB: 模型2': {'acc': 0.8526315789473685,\n",
       "  'auc': 0.9294709884266499,\n",
       "  'mcc': 0.7035907723675942,\n",
       "  'wba': 0.85124204983839,\n",
       "  'ba': 0.8536293791054114,\n",
       "  'f1': 0.8673341029104765},\n",
       " 'XGB: 模型3': {'acc': 0.8501547987616099,\n",
       "  'auc': 0.9329566781357521,\n",
       "  'mcc': 0.7074625922113771,\n",
       "  'wba': 0.8406253258262953,\n",
       "  'ba': 0.8569977713481389,\n",
       "  'f1': 0.859886906663094},\n",
       " 'XGB: 模型4': {'acc': 0.8529411764705882,\n",
       "  'auc': 0.9332171762589928,\n",
       "  'mcc': 0.7084738049477904,\n",
       "  'wba': 0.8470714732561776,\n",
       "  'ba': 0.8571561229277448,\n",
       "  'f1': 0.8648781590889387},\n",
       " 'XGB: 模型5': {'acc': 0.8507739938080496,\n",
       "  'auc': 0.925730665467626,\n",
       "  'mcc': 0.6974893758891944,\n",
       "  'wba': 0.852314670003128,\n",
       "  'ba': 0.849667657178605,\n",
       "  'f1': 0.8674682537652503},\n",
       " 'ANN: 模型1': {'acc': 0.8068111455108358,\n",
       "  'auc': 0.8865332733812948,\n",
       "  'mcc': 0.6191510512770548,\n",
       "  'wba': 0.8001915858617453,\n",
       "  'ba': 0.8115645527056616,\n",
       "  'f1': 0.8207924136179237},\n",
       " 'ANN: 模型2': {'acc': 0.8061919504643962,\n",
       "  'auc': 0.8835627150453551,\n",
       "  'mcc': 0.6406011991504822,\n",
       "  'wba': 0.7856844958815556,\n",
       "  'ba': 0.8209180481701595,\n",
       "  'f1': 0.8078065042569624},\n",
       " 'ANN: 模型3': {'acc': 0.8035603715170279,\n",
       "  'auc': 0.8852596183922428,\n",
       "  'mcc': 0.6130739802107307,\n",
       "  'wba': 0.7957134292565947,\n",
       "  'ba': 0.8091951438848921,\n",
       "  'f1': 0.8167322276208939}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bde3ff59-8896-433f-afc2-8654f5f18b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 在训练集上训练模型\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 在测试集上评估模型\u001b[39;00m\n\u001b[0;32m     17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\svm\\_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 252\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\svm\\_base.py:331\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    317\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    321\u001b[0m (\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 331\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# 假设 datasets 是一个包含所有数据集的列表，每个数据集都是一个字典，包含 'X_train', 'y_train', 'X_test', 'y_test'\n",
    "for model_name, model_list in configured_models.items():\n",
    "    results[model_name] = []\n",
    "    for model in model_list:\n",
    "        # 存储当前模型的所有评分\n",
    "        scores = {key: [] for key in scoring}\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n",
    "            \n",
    "            # 在训练集上训练模型\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # 在测试集上评估模型\n",
    "            predictions = model.predict(X_test)\n",
    "            probabilities = model.predict_proba(X_test)[:,1] \n",
    "\n",
    "            for name, scorer in scoring.items():\n",
    "                if 'auc' in name and probabilities is not None:\n",
    "                    # 对于需要预测概率的评分指标，如 AUC\n",
    "                    score_value = scorer._score_func(y_test, probabilities, **scorer._kwargs)\n",
    "                else:\n",
    "                    # 对于不需要预测概率的评分指标\n",
    "                    score_value = scorer._score_func(y_test, predictions, **scorer._kwargs)\n",
    "                scores[name].append(score_value)\n",
    "        \n",
    "        # 计算每个指标的平均分数\n",
    "        average_scores = {key: np.mean(value) for key, value in scores.items()}\n",
    "        results[model_name].append(average_scores)\n",
    "\n",
    "# 最后，results 字典包含了每种模型在测试集上的平均评分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f1254d2-7fcf-4eda-8173-28e6100b4989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['y_test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59bac69a-c48f-43ac-8067-3268eb0538fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|                                                                 | 0/10 [00:00<?, ?dataset/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(X_train, y_train)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# 对于每个指标，计算得分并将其添加到相应的列表中\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, scorer \u001b[38;5;129;01min\u001b[39;00m scoring\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "all_average_scores = {}\n",
    "for model_name, model in configured_models:\n",
    "    average_scores = {}\n",
    "    for dataset in tqdm(datasets, desc=\"Processing datasets\", unit=\"dataset\"):\n",
    "        X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train)\n",
    "            # 对于每个指标，计算得分并将其添加到相应的列表中\n",
    "        for metric, scorer in scoring.items():\n",
    "            if metric == 'auc':\n",
    "                # 对于需要概率的评分指标，我们使用 predict_proba\n",
    "                y_proba = lr.predict_proba(X_test)[:1]\n",
    "                score = scorer(lr, X_test, y_test)  # 使用概率来计算分数\n",
    "            else:\n",
    "                # 对于其他指标，我们使用预测的类别\n",
    "                y_pred = lr.predict(X_test)\n",
    "                score = scorer(lr, X_test, y_test)  # 使用预测来计算分数\n",
    "\n",
    "            scores[metric].append(score)\n",
    "\n",
    "    # 计算每个指标的平均分数\n",
    "    average_scores = {metric: np.mean(score_values) for metric, score_values in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "209b2d4f-f31f-4c9a-ad55-4979bc45177b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "SVM\n",
      "RF\n",
      "XGB\n",
      "ANN\n"
     ]
    }
   ],
   "source": [
    "for configured_model in configured_models:\n",
    "    print(configured_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc8f19e0-d8ca-49e2-9212-cc996ee4c807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LR...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1\n 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1\n 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0\n 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0\n 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1\n 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1\n 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0\n 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1\n 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1\n 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0\n 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1\n 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, scorer \u001b[38;5;129;01min\u001b[39;00m scoring\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# 我们已经处理过了 AUC\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m             score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m             scores[metric]\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 计算平均分数并存储结果\u001b[39;00m\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:234\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, estimator, X, y_true, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:276\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_caller, estimator, X, y_true, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_caller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(\n\u001b[0;32m    279\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\n\u001b[0;32m    280\u001b[0m         )\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:73\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[method]\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mH:\\ProgramData\\Anaconda\\envs\\psap-torch\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1\n 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1\n 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0\n 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0\n 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1\n 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1\n 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0\n 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1\n 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1\n 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0\n 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1\n 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, f1_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 准备一个字典来收集每个模型的结果\n",
    "results = {}\n",
    "\n",
    "# 遍历每种类型的模型配置\n",
    "for model_type, model_list in configured_models.items():\n",
    "    print(f\"Processing {model_type}...\")\n",
    "\n",
    "    # 对于每种类型的每个模型配置，我们将其结果存储在此列表中\n",
    "    type_results = []\n",
    "\n",
    "    for model in model_list:\n",
    "        # 用于存储当前模型配置的各种评分\n",
    "        scores = {key: [] for key in scoring}\n",
    "\n",
    "        # 假设 'datasets' 是您的多个数据集的列表\n",
    "        for dataset in datasets:\n",
    "            X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n",
    "\n",
    "            # 训练模型并进行评分\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # 如果评分机制需要概率或决策函数，我们这里处理\n",
    "            if 'auc' in scoring:\n",
    "                # 有些模型方法可能没有 'predict_proba'\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_score = model.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(model, 'decision_function'):\n",
    "                    y_score = model.decision_function(X_test)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Model {model_type} does not have 'predict_proba' or 'decision_function' method required for 'auc' scoring.\")\n",
    "\n",
    "                scores['auc'].append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "            # 计算其他指标\n",
    "            for metric, scorer in scoring.items():\n",
    "                if metric != 'auc':  # 我们已经处理过了 AUC\n",
    "                    score = scorer(model, y_test, y_pred)\n",
    "                    scores[metric].append(score)\n",
    "\n",
    "        # 计算平均分数并存储结果\n",
    "        avg_scores = {metric: np.mean(values) for metric, values in scores.items()}\n",
    "        type_results.append(avg_scores)\n",
    "\n",
    "    # 将结果存储到主结果字典中\n",
    "    results[model_type] = type_results\n",
    "\n",
    "# 输出结果\n",
    "for model_type, type_results in results.items():\n",
    "    print(f\"Results for {model_type}:\")\n",
    "    for config_results in type_results:\n",
    "        print(config_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8d5bb-3fad-478f-a3ab-3d8f5c1464d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
