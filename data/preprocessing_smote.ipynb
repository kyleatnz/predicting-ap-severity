{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f2e29-eee6-400d-bbd3-155ea19c67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3d7b0-b483-497d-8110-c5b41dff38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(SimpleImputer):\n",
    "    def fit(self, X, y=None):\n",
    "        super().fit(X, y)\n",
    "        # Storing the column names\n",
    "        self.column_names = X.columns  \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data = super().transform(X)\n",
    "        # Converting the ndarray back to a DataFrame and assigning the column names\n",
    "        return pd.DataFrame(data, columns=self.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f6ce4-3adb-42a4-ad47-d956369368b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, log_cols_idx, scale_cols_idx, column_names=None):\n",
    "        self.log_cols_idx = log_cols_idx\n",
    "        self.scale_cols_idx = scale_cols_idx\n",
    "        self.column_names = column_names \n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.column_names = X.columns.tolist()\n",
    "        self.scaler.fit(X.iloc[:, self.scale_cols_idx])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.copy() \n",
    "        X_transformed.iloc[:, self.log_cols_idx] = np.log1p(X_transformed.iloc[:, self.log_cols_idx])\n",
    "        X_transformed.iloc[:, self.scale_cols_idx] = self.scaler.transform(X_transformed.iloc[:, self.scale_cols_idx])\n",
    "        return pd.DataFrame(X_transformed, columns=self.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a3a90-69c7-4190-a8b3-d3661484a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "    ('imputer', DataFrameImputer(strategy='mean')), \n",
    "    ('transformer', CustomTransformer(log_cols_idx=None, scale_cols_idx=None))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e99efd-6671-4fb4-aa54-a31daf3e2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(self, file, y_column, test_size, task, preprocessor, num_splits=10, random_state=2023):\n",
    "        self.file = file\n",
    "        self.y_column = y_column\n",
    "        self.test_size = test_size\n",
    "        self.task = task\n",
    "        self.random_state = random_state\n",
    "        self.data = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.preprocessor = preprocessor\n",
    "        self.num_splits = num_splits\n",
    "        self.preprocessed_data_splits = [] \n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file)\n",
    "        self.column_names = self.data.columns.tolist()\n",
    "    \n",
    "    def get_X_y(self):\n",
    "        df = self.data.copy()\n",
    "        self.X = df.drop(self.y_column, axis=1)\n",
    "        self.y = df[self.y_column]\n",
    "        # binary classification scenarios\n",
    "        # severe & moderate = 1 vs. mild\n",
    "        if self.task == \"binary-0\":\n",
    "            self.y = self.y.apply(lambda x: 0 if x == 0 else 1)\n",
    "            self.prevalence = sum(self.y) / len(self.y)\n",
    "        \n",
    "        # mild = 1 vs. severe & moderate\n",
    "        elif self.task == \"binary-1\":\n",
    "            self.y = self.y.apply(lambda x: 1 if x == 0 else 0)\n",
    "            self.prevalence = sum(self.y) / len(self.y)\n",
    "            \n",
    "        # severe = 1 vs. moderate & mild\n",
    "        elif self.task == \"binary-2\":\n",
    "            self.y = self.y.apply(lambda x: 0 if x in [0,1] else 1)\n",
    "            self.prevalence = sum(self.y) / len(self.y)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def log_sca_idx(self):\n",
    "        self.log_cols_idx = [self.X.columns.get_loc(col) for col in log_cols]\n",
    "        self.sca_cols_idx = [self.X.columns.get_loc(col) for col in sca_cols]\n",
    "        self.preprocessor.named_steps['transformer'].log_cols_idx = self.log_cols_idx\n",
    "        self.preprocessor.named_steps['transformer'].scale_cols_idx = self.sca_cols_idx\n",
    "\n",
    "    def create_preprocessed_splits(self):\n",
    "\n",
    "        for i in range(self.num_splits):\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                self.X, self.y, test_size=self.test_size, stratify=self.y, random_state=self.random_state+i)\n",
    "\n",
    "            X_train_preprocessed = self.preprocessor.fit_transform(X_train)\n",
    "            X_test_preprocessed = self.preprocessor.transform(X_test)\n",
    "            \n",
    "            smote = SMOTE(random_state=self.random_state)\n",
    "            X_train_smote, y_train_smote = smote.fit_resample(X_train_preprocessed.copy(), y_train.copy())\n",
    "\n",
    "            self.preprocessed_data_splits.append({\n",
    "                'X_train': X_train_smote,\n",
    "                'X_test': X_test_preprocessed,\n",
    "                'y_train': y_train_smote,\n",
    "                'y_test': y_test\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ed09b-afd5-499f-b85d-be9ffa98b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols = ['Amylase', 'Lipase', 'R', 'WBC', 'NEUT', 'LYMPH', 'PLT', 'HCT', 'ALT', \n",
    "            'AST', 'ALB', 'GLU', 'BUN', 'CR', 'TG', 'CHOL', 'LDH', 'HBDH', 'Ca']\n",
    "sca_cols = ['Age', 'Amylase', 'Lipase', 'Abdominal pain onset time (h)', 'T', 'R', \n",
    "            'HR', 'WBC', 'NEUT', 'LYMPH', 'PLT', 'HCT', 'Hgb', 'ALT', 'AST', 'ALB', \n",
    "            'GLB', 'GLU', 'BUN', 'CR', 'TG', 'CHOL', 'HDL-C', 'LDL-C', 'LDH', \n",
    "            'HBDH', 'Ca', 'CRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9cf78-74cb-4d1e-97cd-2a3fe5dd28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = {\"RAC\": \"data_rac.csv\", \"DBC\": \"data_dbc.csv\", \"MDBC\": \"data_mdbc.csv\"}\n",
    "for key, value in file_name.items():\n",
    "\n",
    "    file = f\"{file_path}{value}\"\n",
    "    data_pipeline = DataPipeline(file=file, y_column=key, test_size=.25, task='mulit', preprocessor=preprocessor)\n",
    "    \n",
    "    # Execute the data processing steps for the current dataset\n",
    "    data_pipeline.load_data()\n",
    "    data_pipeline.get_X_y()\n",
    "    data_pipeline.log_sca_idx()\n",
    "    data_pipeline.create_preprocessed_splits() \n",
    "    \n",
    "    # Save the preprocessed data splits for the current dataset\n",
    "    with open(f'b_{key.lower()}_smote_splits.pkl', 'wb') as f:\n",
    "        pickle.dump(data_pipeline.preprocessed_data_splits, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
